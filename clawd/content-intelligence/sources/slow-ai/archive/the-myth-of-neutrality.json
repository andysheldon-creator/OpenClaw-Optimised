{
  "url": "https://theslowai.substack.com/p/myth-of-ai-neutrality",
  "slug": "the-myth-of-neutrality",
  "title": "The Myth of Neutrality",
  "subtitle": "Generative systems automate historical prejudices by treating biased training data as fundamental laws of reality.",
  "author": "Dr Sam Illingworth",
  "published": "Fri, 30 Jan 2026 10:02:06 GMT",
  "content_html": "<p>Welcome to our first class of the 2026 <em>Slow AI</em> Curriculum for Critical Literacy. </p><p>It was a privilege to see so many of you at our opening webinar. The energy in the room, and the critical scepticism you brought to the dialogue confirms exactly why this community is necessary. For those who joined us live, thank you for your contributions. </p><p>This session explored the statistical illusion of neutrality in AI systems, drawing on a 2023 Lancet study that revealed how image-generating models default to colonial tropes, depicting healers as white and sick people as Black. We tested this live using a prompt asking AI to generate images of a quantum physicist and an unskilled labourer sharing a meal. The results were striking: across tools and continents, participants found remarkably similar biases: the physicist rendered as elderly, white, and male; the labourer marked by grime, high-vis jackets, and subordinate positioning. What emerged from our discussion was not despair but clarity: AI models are prediction engines trained on biased data, and understanding this is the first step toward using them critically rather than passively.</p><p>For those who couldn't make it, <strong>you are exactly where you need to be.</strong> Our curriculum is designed to be a slow, steady immersion. We suggest that those catching up watch the recording below before engaging with this month&#8217;s written inquiry, as it establishes the necessary foundation for our exploration of the &#8216;Myth of Neutrality&#8217;.</p><h4><strong>The webinar </strong></h4>\n      <p>\n          <a href=\"https://theslowai.substack.com/p/myth-of-ai-neutrality\">\n              Read more\n          </a>\n      </p>\n   ",
  "content_text": "Welcome to our first class of the 2026 Slow AI Curriculum for Critical Literacy. It was a privilege to see so many of you at our opening webinar. The energy in the room, and the critical scepticism you brought to the dialogue confirms exactly why this community is necessary. For those who joined us live, thank you for your contributions. This session explored the statistical illusion of neutrality in AI systems, drawing on a 2023 Lancet study that revealed how image-generating models default to colonial tropes, depicting healers as white and sick people as Black. We tested this live using a prompt asking AI to generate images of a quantum physicist and an unskilled labourer sharing a meal. The results were striking: across tools and continents, participants found remarkably similar biases: the physicist rendered as elderly, white, and male; the labourer marked by grime, high-vis jackets, and subordinate positioning. What emerged from our discussion was not despair but clarity: AI models are prediction engines trained on biased data, and understanding this is the first step toward using them critically rather than passively.For those who couldn't make it, you are exactly where you need to be. Our curriculum is designed to be a slow, steady immersion. We suggest that those catching up watch the recording below before engaging with this month’s written inquiry, as it establishes the necessary foundation for our exploration of the ‘Myth of Neutrality’.The webinar \n      \n          \n              Read more\n          \n      \n   ",
  "harvested_at": "2026-01-30T22:02:16.271037"
}